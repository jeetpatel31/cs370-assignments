# -*- coding: utf-8 -*-
"""index.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gpj3zzTURfWtTSQ1GiFTlurEBBBI-Hpc

# Assignment 1  

For the exercises below you can use the numpy and scipy libraries.

## Problem 1: Simulation (20 points)

Review any of the probability theory links [provided in your course site](https://pantelis.github.io/artificial-intelligence/aiml-common/lectures/ml-math/probability/index.html). The exercise refers to Example 6.6 of the  [Math for ML book](https://mml-book.github.io/book/mml-book.pdf).

### Problem 1A (15 points)

Simulate (sample from) the bivariate normal distribution with the shown parameters obtaining a plot similar to Figure 6.8b that shows the simulation result from a different bivariate Gaussian distribution.  You can generate $m=200$ samples/points (10 points)

### Problem 1B (5 points)

Plot the contours of the bivariate Gaussian distribution and the simulated points in the same plot. (5 points)
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

n = 200
m = [0, 0]
cov = [[1, 0.5], [0.5, 1]]

p = np.random.multivariate_normal(m, cov, n)
x, y = np.mgrid[-3:3:.01, -3:3:.01]
pos = np.dstack((x, y))

rv = multivariate_normal(m, cov)
z = rv.pdf(pos)

plt.figure(figsize=(8, 8))
plt.scatter(p[:, 0], p[:, 1], label='Samples')
plt.contour(x, y, z, colors='black')
plt.plot(m[0], m[1], 'rx', label='Mean')
plt.title('Bivariate Gaussian Distribution with Sampled Points')
plt.xlabel('X1')
plt.ylabel('X2')
plt.legend()
plt.grid(True)
plt.show()

"""## Problem 2: Projection (20 points)

You may want to review these [linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) videos or the [other linear algebra links](https://pantelis.github.io/artificial-intelligence/aiml-common/lectures/ml-math/linear-algebra/index.html) provided in your course site.

Simulate a 3-dimensional (3d) Gaussian random vector with the following covariance matrix.

$$
\begin{bmatrix}
4 & 2 & 1 \\
2 & 3 & 1.5 \\
1 & 1.5 & 2 \\
\end{bmatrix}
$$

Using the Singular Value Decomposition (SVD) compute the projection of the simulated vectors onto the subspace spanned by the first two principal components.

### Problem 2A (5 points)

What determines the principal components ?

Principal components analysis looks for the main ways that the data varies, called principal components. It does this by calculating the covariance matrix of the data, which shows how each variable relates to the others. Then it performs singular value decomposition on this matrix, which finds the eigenvectors and eigenvalues. The eigenvectors with the largest eigenvalues are the principal components. The first principal component is the direction with the most variance in the data, the second is the next most important direction orthogonal to the first, and so on. So principal components analysis rotates the data to line up with its most important directions of variation.

### Problem 2B (5 points)

What determines the positive or negative correlations between the components ?

The off-diagonal elements of the covariance matrix show whether principal components are positively or negatively correlated with each other. Positive off-diagonal values mean two components move in the same direction - when one increases, so does the other. Negative values indicate the components move opposite to one another - as one component goes up, the other tends to go down. The size of the off-diagonal elements also represents the strength of the correlation. So if we look at the covariance matrix from principal components analysis, the signs and magnitudes of the off-diagonal entries tell us about the positive or negative correlations between the components. Positive values show positive correlations, negative values show negative correlations, and larger values mean stronger correlations between those components.

### Problem 2C (10 points)

Plot the projected vectors and show whether or not the projection agrees with the positive or negative correlations of the original matrix.


"""

import numpy as np
import matplotlib.pyplot as plt
from numpy.linalg import svd

cov = np.array([[4, 2, 1], [2, 3, 1.5], [1, 1.5, 2]])
n = 200
m = [0, 0, 0]

points_3d = np.random.multivariate_normal(m, cov, n)
U, sigma, Vt = svd(cov)

pc1, pc2 = U[:, 0], U[:, 1]
proj_points = points_3d @ np.column_stack((pc1, pc2))

plt.figure(figsize=(8, 8))
plt.scatter(proj_points[:, 0], proj_points[:, 1], label='Projected Points')
plt.title('Projection onto the Plane of the First Two Principal Components')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.grid(True)
plt.show()

"""### Problem 3: Stochastic Gradient Descent (30 points)

In class we covered the baseline stochastic gradient descent.  Using the linear regression example from the class notes, develop from scratch the baseline SGD algorithm. :

Clearly state the hyperparameters you used and present the loss vs epoch plot that demonstrates the convergence of the algorithm.
"""

# Insert your answer here and fee free to add markdown cells as needed

"""### Problem 4: SGD Enhancements (30 points)

In this exercise you will implement some enhancements for the linear regression problem from scratch that can improve the convergence speed of the algorithm.

1. Momentum (15 points)
2. Adam (15 points)

Clearly state the hyperparameters you used and present the loss vs epoch plot that demonstrates the convergence of each algorithm and compared to the baseline SGD algorithm. You can include all plots in the same figure.
"""

# Insert your answer here and fee free to add markdown cells as needed